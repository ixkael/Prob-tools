{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line to data - a full tutorial\n",
    "\n",
    "_Boris Leistedt, September 2017_\n",
    "\n",
    "This notebook is available at [this location on Github](https://github.com/ixkael/Prob-tools/blob/master/notebooks/Fitting%20a%20line%20to%20data%20-%20a%20full%20tutorial.ipynb).\n",
    "\n",
    "This notebook It assumes some basic knowledge about Bayesian inference and data analysis. It is accompanied with a set of slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config IPython.matplotlib.backend = 'retina'\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Play with the code! Try and do the exercises.\n",
    "\n",
    "Please interrupt me if you are lost or if you disagree with what I say.\n",
    "\n",
    "All questions are welcome, especially the ones that you find \"simple\" (they are often very good, and not simple!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you haven't done it, install those packages using conda and/or pip:\n",
    "\n",
    "``conda install numpy scipy pandas matplotlib jupyter pip``\n",
    "\n",
    "``pip install emcee corner``\n",
    "\n",
    "start a jupyter kernel: ``jupyter notebook``\n",
    "\n",
    "and open a copy of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "matplotlib.rc(\"font\", family=\"serif\", size=14)\n",
    "matplotlib.rc(\"figure\", figsize=\"10, 5\")\n",
    "colors = ['k', 'c', 'm', 'y']\n",
    "matplotlib.rc('axes', prop_cycle=cycler(\"color\", colors))\n",
    "\n",
    "import scipy.optimize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Bayesian inference?\n",
    "\n",
    "Constrain model parameters with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a line to data: basic setup\n",
    "\n",
    "Our model will consist of a set of $N$ i.i.d. observations, including: a coordinate (fixed parameter) $x_i$, a noise level  $\\sigma_i$, and observed variable $\\hat{y}_i$ drawn from a Gaussian with mean $mx_i + b$ and variance $\\sigma^2_i$, i.e. $\\hat{y}_i \\sim \\mathcal{N}(mx_i+b;\\sigma_i^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "slope_true = np.random.uniform(0, 1)\n",
    "intercept_true = np.random.uniform(0, 1)\n",
    "print('Slopes:', slope_true)\n",
    "print('Intercepts:', intercept_true)\n",
    "# This notebook is ready for you to play with 2+ components and more complicated models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate some data drawn from that model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ndatapoints = 20\n",
    "xis_true = np.random.uniform(0, 1, ndatapoints)\n",
    "x_grid = np.linspace(0, 1, 100)\n",
    "\n",
    "\n",
    "def model_linear(xs, slope, intercept): return xs * slope + intercept\n",
    "yis_true = model_linear(xis_true, slope_true, intercept_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sigma_yis = np.repeat(0.1, ndatapoints) * np.random.uniform(0.5, 2.0, ndatapoints)\n",
    "yis_noisy = yis_true + np.random.randn(ndatapoints) * sigma_yis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y_min, y_max = np.min(yis_noisy - sigma_yis), np.max(yis_noisy + sigma_yis)\n",
    "y_min = np.min([y_min, np.min(model_linear(x_grid, slope_true, intercept_true))])\n",
    "y_max = np.max([y_max, np.max(model_linear(x_grid, slope_true, intercept_true))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_grid, model_linear(x_grid, slope_true, intercept_true), c=colors[0])\n",
    "plt.errorbar(xis_true, yis_noisy, sigma_yis, fmt='o', c=colors[0])\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$'); plt.ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to pretend we don't know the true model. \n",
    "\n",
    "Forget what you saw (please)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the noisy data to be analyzed. Can you (mentally) fit a line through it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.errorbar(xis_true, yis_noisy, sigma_yis, fmt='o')\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$'); plt.ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's define a loss/cost function: the total weighted squared error, also called chi-squared: \n",
    "$$ \\chi^2 = \\sum_i \\left( \\frac{ \\hat{y}_i - y_i^\\mathrm{mod}(x_i, s, m) }{\\sigma_i} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def loss(observed_yis, yi_uncertainties, model_yis):\n",
    "    scaled_differences = (observed_yis - model_yis) / yi_uncertainties\n",
    "    return np.sum(scaled_differences**2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want to minimize this chi-squared to obtain the best possible fit to the data. \n",
    "\n",
    "Let us look at the fit for a couple of (random) sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "random_slopes = np.array([0.25, 0.25, 0.75, 0.75])\n",
    "random_intercepts = np.array([0.25, 0.75, 0.25, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True)\n",
    "axs[0].errorbar(xis_true, yis_noisy, sigma_yis, fmt='o')\n",
    "for i, (slope, intercept) in enumerate(zip(random_slopes, random_intercepts)):\n",
    "    axs[0].plot(x_grid, model_linear(x_grid, slope, intercept), c=colors[i])\n",
    "    axs[1].scatter(slope, intercept,marker='x', c=colors[i])\n",
    "    chi2 = loss(yis_noisy[:, None], sigma_yis[:, None], \n",
    "                 model_linear(xis_true[:, None], slope, intercept))\n",
    "    axs[1].text(slope, intercept+0.05, '$\\chi^2 = %.1f$'% chi2, \n",
    "                horizontalalignment='center')\n",
    "axs[0].set_xlabel('$x$'); axs[0].set_ylabel('$y$')\n",
    "axs[0].set_ylim([0, y_max]); axs[1].set_ylim([0, 1]); \n",
    "axs[1].set_xlabel('slope'); axs[1].set_ylabel('intercept')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us try a brute-force search, and grid our 2D parameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Create a 100 x 100 grid covering our parameter space. \n",
    "\n",
    "Evaluate the loss function on the grid, and plot exp(-0.5*loss).\n",
    "\n",
    "Also find the point that has the minimal loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=False, sharey=False)\n",
    "axs[0].errorbar(xis_true, yis_noisy, sigma_yis, fmt='o')\n",
    "axs[0].plot(x_grid, model_linear(x_grid, slope_ml, intercept_ml))\n",
    "axs[0].set_xlabel('$x$'); axs[0].set_ylabel('$y$')\n",
    "axs[0].set_ylim([y_min, y_max])\n",
    "axs[1].set_xlabel('slope'); axs[1].set_ylabel('intercept')\n",
    "axs[1].axvline(slope_ml, c=colors[1]); axs[1].axhline(intercept_ml, c=colors[1])\n",
    "axs[1].pcolormesh(slope_grid, intercept_grid, np.exp(-0.5*loss_grid), cmap='ocean_r')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why visualize $exp(-\\frac{1}{2}\\chi^2)$ and not simply the $\\chi^2$?\n",
    "\n",
    "Because the former is proportional to our likelihood:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D| P, M) &= p(\\{ \\hat{y}_i \\} \\vert \\{\\sigma_i, x_i\\}, \\textrm{intercept}, \\textrm{slope}) \\\\\n",
    "&= \\prod_{i=1}^{N} p(\\hat{y}_i \\vert x_i, \\sigma_i, b, m)\\\\\n",
    "&= \\prod_{i=1}^{N} \\mathcal{N}\\left(\\hat{y}_i - y^\\mathrm{mod}(x_i; m, b); \\sigma^2_i \\right)\n",
    "\\ = \\prod_{i=1}^{N} \\mathcal{N}\\left(\\hat{y}_i - m x_i - b; \\sigma^2_i \\right) \\\\\n",
    "&= \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi}\\sigma_i}\\exp\\left( - \\frac{1}{2} \\frac{(\\hat{y}_i - m x_i - b)^2}{\\sigma^2_i} \\right)  \\\\\n",
    "&\\propto \\ \\exp\\left( - \\sum_{i=1}^{N} \\frac{1}{2} \\frac{(\\hat{y}_i - m x_i - b)^2}{\\sigma^2_i} \\right) \\ = \\ \\exp\\left(-\\frac{1}{2}\\chi^2\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the data points are independent and the noise is Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the $\\chi^2$ for individual objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model_yis = model_linear(xis_true, slope_ml, intercept_ml)\n",
    "object_chi2s = 0.5*((yis_noisy - model_yis) / sigma_yis)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x_grid, model_linear(x_grid, slope_ml, intercept_ml))\n",
    "v = ax.scatter(xis_true, yis_noisy, c=object_chi2s, cmap='coolwarm', zorder=0)\n",
    "ax.errorbar(xis_true, yis_noisy, sigma_yis, fmt='o', zorder=-1)\n",
    "ax.set_xlabel('$x$'); ax.set_ylabel('$y$'); ax.set_ylim([y_min, y_max])\n",
    "plt.colorbar(v); fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Digression: the limits of maximum likelihood\n",
    "\n",
    "Is a line a good model? \n",
    "\n",
    "Should we aiming at maximizing the likelihood only?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a danger of Maximum Likelihood: there is always of model that perfectly fits all of the data.\n",
    "    \n",
    "This model does not have to be complicated..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE (5 min): can you try to write a very flexible model that fits the data perfectly, i.e. go through every single point? What $\\chi^2$ does it lead to? \n",
    "\n",
    "NOTE: this might not be trivial, so just look for a model that goes through *most* of the data points.\n",
    "\n",
    "HINT: numpy has good infrastructure for constructing and fitting polynomials... (try `?np.polyfit`).\n",
    "\n",
    "If you pick a more complicated model you might need to use `scipy.optimize.minimize`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_grid, bestfit_polynomial(x_grid))\n",
    "plt.errorbar(xis_true, yis_noisy, sigma_yis, fmt='o')\n",
    "plt.ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Bayes' theorem \n",
    "with explicit Model and Fixed parameters conditioned on:\n",
    "\n",
    "$$p(P | D, M, F) = \\frac{p(D | P, M, F)\\ p(P | M, F)}{p(D | M, F)}$$\n",
    "\n",
    "In our case, if we omit the explicit dependence on a linear model:\n",
    "\n",
    "$$p\\bigl(m, b \\ \\bigl\\vert \\ \\{ \\hat{y}_i, \\sigma_i, x_i\\} \\bigr) \\ \\propto \\ p\\bigl(\\{ \\hat{y}_i \\} \\ \\bigl\\vert \\ m, b, \\{\\sigma_i, x_i\\}\\bigr) \\  p\\bigl(m, b\\bigr) \\ = \\ \\exp\\bigl(-\\frac{1}{2}\\chi^2\\bigr)\\ p\\bigl(m, b\\bigr) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let us play with Bayes theorem and pick some un-motivated prior:\n",
    "prior_grid = np.exp(-slope_grid**-1) * np.exp(-intercept_grid**-1)\n",
    "likelihood_grid = np.exp(-0.5*loss_grid)\n",
    "posterior_grid = likelihood_grid * prior_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3)\n",
    "for i in range(3):\n",
    "    axs[i].set_ylabel('intercept'); axs[i].set_xlabel('slope'); \n",
    "axs[0].set_title('Prior'); axs[1].set_title('Likelihood'); axs[2].set_title('Posterior')\n",
    "axs[1].axvline(slope_ml, c=colors[1]); axs[1].axhline(intercept_ml, c=colors[1])\n",
    "axs[0].pcolormesh(slope_grid, intercept_grid, prior_grid, cmap='ocean_r')\n",
    "axs[1].pcolormesh(slope_grid, intercept_grid, likelihood_grid, cmap='ocean_r')\n",
    "axs[2].pcolormesh(slope_grid, intercept_grid, posterior_grid, cmap='ocean_r')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Discussion: what priors are adequate here? \n",
    "\n",
    "Four common types of priors are:\n",
    "- __Flat priors__: uniform probability in $[0, 1]$ for both the slope and intercept.\n",
    "- __Conjugate priors__: Gaussians or inverse Gamma...\n",
    "- __Empirical priors__: previous experiments told me that $p(m) = \\mathcal{N}(0.5, 0.125)$ and $p(b) = \\mathcal{N}(0.5, 0.125)$.\n",
    "- __Non-informative priors__: rotationally invariance for the line: $p(m) \\propto (1 + m^2)^{-1.5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## The impact of priors\n",
    "\n",
    "prior_grid3 = (1 + slope_grid**2)**-1.5\n",
    "prior_grid2 = np.exp(-0.5*((slope_grid-0.5)/0.125)**2.0) * np.exp(-0.5*((intercept_grid-0.5)/0.125)**2.0)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for i in range(3):\n",
    "    axs[1, i].set_xlabel('slope'); \n",
    "    for j in range(2):\n",
    "        axs[j, 0].set_ylabel('intercept'); \n",
    "        axs[j, i].axvline(slope_ml, c=colors[1]); \n",
    "        axs[j, i].axhline(intercept_ml, c=colors[1])\n",
    "    intercept_vals = intercept_grid[likelihood_grid > likelihood_grid.max() / 10.]\n",
    "    slope_vals = slope_grid[likelihood_grid > likelihood_grid.max() / 10.]\n",
    "    axs[1, i].set_xlim([slope_vals.min(), slope_vals.max()])\n",
    "    axs[1, i].set_ylim([intercept_vals.min(), intercept_vals.max()])\n",
    "\n",
    "axs[1, 0].pcolormesh(slope_grid, intercept_grid, likelihood_grid, cmap='ocean_r')\n",
    "axs[0, 0].set_title('Flat prior')\n",
    "\n",
    "axs[0, 1].pcolormesh(slope_grid, intercept_grid, prior_grid2, cmap='ocean_r')\n",
    "axs[1, 1].pcolormesh(slope_grid, intercept_grid, likelihood_grid*prior_grid2, cmap='ocean_r')\n",
    "axs[0, 1].set_title('Experimental prior')\n",
    "\n",
    "axs[0, 2].pcolormesh(slope_grid, intercept_grid, prior_grid3, cmap='ocean_r')\n",
    "axs[1, 2].pcolormesh(slope_grid, intercept_grid, likelihood_grid*prior_grid3, cmap='ocean_r')\n",
    "axs[0, 2].set_title('Non-informative prior')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "normalization2 = (prior_grid2).max()\n",
    "normalization3 = (prior_grid3).max()\n",
    "\n",
    "num_draws = 200\n",
    "params_drawn1 = np.random.uniform(0, 1, num_draws*2).reshape((num_draws, 2))\n",
    "params_drawn2 = np.zeros((num_draws, 2))\n",
    "params_drawn3 = np.zeros((num_draws, 2))\n",
    "\n",
    "i_draw = 0\n",
    "num_tot = 0\n",
    "while i_draw < num_draws:\n",
    "    params_drawn2[i_draw, :] = np.random.uniform(0, 1, 2)\n",
    "    pri = np.exp(-0.5*((params_drawn2[i_draw, 0]-0.5)/0.125)**2.0) * np.exp(-0.5*((params_drawn2[i_draw, 1]-0.5)/0.125)**2.0)\n",
    "    num_tot += 1\n",
    "    if np.random.uniform(0, 1, 1) < pri / normalization2:\n",
    "        i_draw += 1\n",
    "print(num_tot, num_draws)\n",
    "\n",
    "i_draw = 0\n",
    "while i_draw < num_draws:\n",
    "    params_drawn3[i_draw, :] = np.random.uniform(0, 1, 2)\n",
    "    pri = (1 + params_drawn3[i_draw, 0]**2)**-1.5\n",
    "    num_tot += 1\n",
    "    if np.random.uniform(0, 1, 1) < pri / normalization3:\n",
    "        i_draw += 1\n",
    "print(num_tot, num_draws)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(9, 6))\n",
    "for i in range(3):\n",
    "    axs[0, i].set_xlabel('slope'); \n",
    "    axs[0, i].set_ylabel('intercept');\n",
    "    axs[1, i].set_xlabel('$x$'); \n",
    "    axs[1, i].set_ylabel('$y$');\n",
    "\n",
    "axs[0, 0].scatter(params_drawn1[:, 0], params_drawn1[:, 1], cmap='ocean_r')\n",
    "axs[0, 0].set_title('Flat prior')\n",
    "\n",
    "axs[0, 1].pcolormesh(slope_grid, intercept_grid, prior_grid2, cmap='ocean_r')\n",
    "axs[0, 1].scatter(params_drawn2[:, 0], params_drawn2[:, 1], cmap='ocean_r')\n",
    "axs[0, 1].set_title('Experimental prior')\n",
    "\n",
    "axs[0, 2].pcolormesh(slope_grid, intercept_grid, prior_grid3, cmap='ocean_r')\n",
    "axs[0, 2].scatter(params_drawn3[:, 0], params_drawn3[:, 1], cmap='ocean_r')\n",
    "axs[0, 2].set_title('Non-informative prior')\n",
    "\n",
    "for i_draw in range(num_draws):\n",
    "    axs[1, 0].plot(x_grid, model_linear(x_grid, params_drawn1[i_draw, 0], params_drawn1[i_draw, 1]), c='grey', alpha=0.1)\n",
    "    axs[1, 1].plot(x_grid, model_linear(x_grid, params_drawn2[i_draw, 0], params_drawn2[i_draw, 1]), c='grey', alpha=0.1)\n",
    "    axs[1, 2].plot(x_grid, model_linear(x_grid, params_drawn3[i_draw, 0], params_drawn3[i_draw, 1]), c='grey', alpha=0.1)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Curse of Dimensionality (v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problems with 'gridding': number of likelihood evaluations, resolution of the grids, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "ax.set_xlabel('slope'); ax.set_ylabel('intercept');\n",
    "ax.scatter(slope_grid.ravel(), intercept_grid.ravel(), marker='.', s=1)\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlim([0, 1])\n",
    "fig.tight_layout()\n",
    "print('Number of point/evaluations of the likelihood:', slope_grid.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note to Boris: Go back to slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling posterior distributions with MCMC\n",
    "\n",
    "We are going to approximate the posterior distribution with a set of samples (see slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Write three functions returning:\n",
    "        \n",
    "- the log of the likelihood `ln_like(params, args...)`.\n",
    "\n",
    "- the log of the prior `ln_prior(params, args...)`.\n",
    "\n",
    "- the log of the posterior `ln_post(params, args...)`.\n",
    "\n",
    "\n",
    "The likelihood is pretty much our previous loss function.\n",
    "\n",
    "The prior should return `-np.inf` outside of our parameter space of interest. At this stage use a uniform prior in $[0, 1] \\times [0, 1]$. \n",
    "\n",
    "Think about what other priors could be used. Include the correct normalization in the prior and the likelihood if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def ln_like(params, xs, observed_yis, yi_uncertainties):\n",
    "    model_yis = model_linear(xs, params[0], params[1])\n",
    "    chi2s = ((observed_yis - model_yis) / yi_uncertainties)**2\n",
    "    return np.sum(-0.5 * chi2s - 0.5*np.log(2*np.pi) - np.log(yi_uncertainties))\n",
    "\n",
    "def ln_prior(params):\n",
    "    if np.any(params < 0) or np.any(params > 1):\n",
    "        return - np.inf\n",
    "    return 0.\n",
    "\n",
    "def ln_post(params, xs, observed_yis, yi_uncertainties):\n",
    "    lnprior_val = ln_prior(params)\n",
    "    if ~np.isfinite(lnprior_val):\n",
    "        return lnprior_val\n",
    "    else:           \n",
    "        lnlike_val = ln_like(params, xs, observed_yis, yi_uncertainties)\n",
    "        return lnprior_val + lnlike_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.array([0.5, 0.5])\n",
    "print('Likelihood:', ln_like(x0, xis_true, yis_noisy, sigma_yis))\n",
    "print('Prior:', ln_prior(x0))\n",
    "print('Posterior:', ln_post(x0, xis_true, yis_noisy, sigma_yis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE (2 min)\n",
    "\n",
    "Find the maximum of the log posterior. Try different optimizers in `scipy.optimize.minimize`. Be careful about the sign of the objective function (is it plus or minus the log posterior?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling strategy 1: Rejection Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Implement rejection sampling. Randomly draw points in our 2D parameter space. Keep each point with a probability proportional to the posterior distribution.\n",
    "\n",
    "HINT: you will find that you need to normalize the posterior distribution in some way to make the sampling possible. Use the MAP solution we just found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].pcolormesh(slope_grid, intercept_grid, likelihood_grid, cmap='ocean_r')\n",
    "axs[1].hist2d(params_drawn[:, 0], params_drawn[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[0].set_title('Gridding'); axs[1].set_title('Rejection sampling'); \n",
    "axs[0].set_xlabel('slope'); axs[0].set_ylabel('intercept'); axs[1].set_xlabel('slope'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling strategy 2: Metropolis-Hastings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Metropolis-Hastings algorithm\n",
    "\n",
    "For a given target probability $p(\\theta)$ and a (symmetric) proposal density $p(\\theta_{i+1}|\\theta_i)$. We repeat the following: \n",
    "- draw a sample $\\theta_{i+1}$ given $\\theta_i$ from the proposal density, \n",
    "- compute the acceptance probability ratio $a={p(\\theta_{i+1})}/{p(\\theta_i)}$, \n",
    "- draw a random uniform number $r$ in $[0, 1]$ and accept $\\theta_{i+1}$ if $r < a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Use your implementation of the Metropolis-Hastings algorithm to draw samples from our 2D posterior distribution of interest.\n",
    "\n",
    "Measure the proportion of parameter draws that are accepted: the acceptance rate.\n",
    "\n",
    "Plot the chain and visualize the burn-in phase.\n",
    "\n",
    "Compare the sampling to our previous gridded version.\n",
    "\n",
    "Estimate the mean and standard deviation of the distribution from the samples. Are they accurate? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].pcolormesh(slope_grid, intercept_grid, likelihood_grid, cmap='ocean_r')\n",
    "axs[1].hist2d(params_drawn[:, 0], params_drawn[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[0].set_title('Gridding'); axs[1].set_title('Metropolis Hastings'); \n",
    "axs[0].set_xlabel('slope'); axs[0].set_ylabel('intercept'); axs[1].set_xlabel('slope'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(params_drawn[:, i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Validation\n",
    "\n",
    "MCMC is approximate and is only valid if it has converged. But we can't prove that a chain has converget - we can only show it hasn't.\n",
    "\n",
    "What to do? ___Be paranoïd.__\n",
    "\n",
    "Is it crucial to 1) run many chains in various setups, and 2) check that the results are stable, and 3) look at the auto-correlation time:\n",
    "\n",
    "$$\\rho_k = \\frac{\\mathrm{Covar}[X_t, X_{t+k}]}{\\mathrm{Var}[X_t]\\mathrm{Var}[X_{t+k}]]}$$\n",
    "\n",
    "See http://rstudio-pubs-static.s3.amazonaws.com/258436_5c7f6f9a84bd47aeaa33ee763e57a531.html and  www.astrostatistics.psu.edu/RLectures/diagnosticsMCMC.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Visualize chains, autocorrelation time, etc, for short and long chains with different proposal distributions in the Metropolis Hastings algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    plt.plot(autocorr_naive(params_drawn[:, i], 500))\n",
    "plt.xscale('log'); plt.xlabel('$\\Delta$'); plt.ylabel('Autocorrelation'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sampling strategy 3: affine-invariant ensemble sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Let's use a more advanced sampler. Look at the documentation of the `emcee` package and use it to (again) draw samples from our 2D posterior distribution of interest. Make 2D plots with both `plt.hist2d` or `plt.contourf`. For the latter, add 68% and 95% confidence contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "for i in range(axs.size):\n",
    "    axs[i].errorbar(sampler.chain[:, i, 0], sampler.chain[:, i, 1], fmt=\"-o\", alpha=0.5, c='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "for i in range(axs.size):\n",
    "    axs[i].errorbar(sampler.chain[:, i, 0], sampler.chain[:, i, 1], fmt=\"-o\", alpha=0.5, c='k');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from corner import hist2d\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].hist2d(sampler.flatchain[:, 0], sampler.flatchain[:, 1], 30, cmap=\"ocean_r\");\n",
    "hist2d(sampler.flatchain[:, 0], sampler.flatchain[:, 1], ax=axs[1])\n",
    "axs[0].set_xlabel('slope'); axs[0].set_ylabel('intercept'); axs[1].set_xlabel('slope');\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].hist(sampler.flatchain[:, 0], histtype='step');\n",
    "axs[1].hist(sampler.flatchain[:, 1], histtype='step');\n",
    "axs[0].set_xlabel('slope'); axs[1].set_xlabel('intercept'); axs[0].set_ylabel('Marginal distribution'); \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is extremely useful to plot the model in data space!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Loop through the posterior samples (a random subset of them?) and over-plot them with the data, with some transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].hist(sampler.flatchain[:, 0], histtype='step');\n",
    "axs[1].hist(sampler.flatchain[:, 1], histtype='step');\n",
    "axs[0].set_xlabel('slope'); axs[1].set_xlabel('intercept'); axs[0].set_ylabel('Marginal distribution'); \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter estimation\n",
    "\n",
    "Often, we want to report summary statistics on our parameters, e.g. in a paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE\n",
    "\n",
    "Compute some useful summary statistics for our two parameters from the MCMC chains: mean, confidence intervals, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "NOTE: for any subsequent analysis, don't use the summary statistics, use the full MCMC chains if you can!\n",
    "\n",
    "CONTROVERSIAL: if you are only ever going to report and use the mean of a parameter, maybe you don't need MCMC... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gibbs sampling\n",
    "\n",
    "Because of the factorization $p(m, b) = p(m | b)p(b) = p(b|m) p(m)$ we can actually design an algorithm to create an MCMC chain where there is __no need to reject points__! \n",
    "\n",
    "The procedure is simple: __given__ $(m_i, b_i)$, __draw__ $m_{i+1}$ from $p(m | b=b_i)$, then __draw__ $b_{i+1}$ from $p(b | m=m_{i+1})$.\n",
    "\n",
    "The only __strong condition__ is to be able to draw directly from the conditional distributions, here $p(m | b)$ and $p(b|m)$.\n",
    "\n",
    "Generalized to 3+ variables and blocks: draw from (block) conditional distributions in a sequence.\n",
    "\n",
    "I will draw the way Gibbs sampling works on the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What conditional distributions do we need here? Recall that our full posterior distribution is\n",
    "\n",
    "$$p\\bigl(m, b \\ \\bigl\\vert \\ \\{ \\hat{y}_i, \\sigma_i, x_i\\} \\bigr) \\ \\propto \\ p\\bigl(\\{ \\hat{y}_i \\} \\ \\bigl\\vert \\ m, b, \\{\\sigma_i, x_i\\}\\bigr) \\  p\\bigl(m, b\\bigr) $$\n",
    "$$= \\ \\exp\\left( - \\sum_{i=1}^{N} \\frac{1}{2} \\frac{(\\hat{y}_i - m x_i - b)^2}{\\sigma^2_i} \\right) \\ p\\bigl(m, b\\bigr) $$\n",
    "\n",
    "If we take uniform priors, $p\\bigl(m, b\\bigr) = C$, then this posterior distribution is Gaussian:\n",
    "\n",
    "$$p\\bigl(m, b \\ \\bigl\\vert \\ \\{ \\hat{y}_i, \\sigma_i, x_i\\} \\bigr) \\ \\propto \\ \\mathcal{N}\\left( [X^TX]^{-1}X^T Y ; [X^TX]^{-1} \\Sigma \\right) $$\n",
    "\n",
    "$$= \\mathcal{N}\\left( \n",
    "\\left[\\begin{matrix} \\hat{m} \\\\ \\hat{b} \\end{matrix}\\right];\n",
    "\\left[\\begin{matrix} \\Sigma_{mm} & \\Sigma_{mb}\\\\ \\Sigma_{mb} & \\Sigma_{bb} \\end{matrix}\\right]\n",
    "\\right) $$\n",
    "\n",
    "where I have put the $x_i$'s in a $N x 2$ vector $X$ (the first column is ones, and the second contains the $x_i$'s),  the $\\hat{y}_i$'s in a vector $Y$, and the $\\sigma_i$'s in a diagonal matrix $\\Sigma$. This matches the classic maximum likelihood results for linear regression. The variables $(\\hat{m}, \\hat{b}, \\Sigma_{mm}, \\Sigma_{mb}, \\Sigma_{mb}, \\Sigma_{bb})$ are a convenient notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By using Gaussian identities (which can for example be found [here](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)), we can write the conditional distributions \n",
    "\n",
    "$$p\\bigl(m \\ \\bigl\\vert \\ b, \\{ \\hat{y}_i, \\sigma_i, x_i\\} \\bigr) \\ \\propto \\mathcal{N}\\left( \n",
    "\\hat{m} + \\Sigma_{mb}\\Sigma_{bb}^{-1}(b - \\hat{b});\n",
    "\\Sigma_{mm} - \\Sigma_{mb}\\Sigma_{bb}^{-1}\\Sigma_{mb}\n",
    "\\right) $$\n",
    "\n",
    "$$p\\bigl(b \\ \\bigl\\vert \\ m, \\{ \\hat{y}_i, \\sigma_i, x_i\\} \\bigr) \\ \\propto \\mathcal{N}\\left( \n",
    "\\hat{b} + \\Sigma_{mb}\\Sigma_{mm}^{-1}(m - \\hat{m});\n",
    "\\Sigma_{bb} - \\Sigma_{mb}\\Sigma_{mm}^{-1}\\Sigma_{mb}\n",
    "\\right) $$\n",
    "\n",
    "__Important__: It is because we know how to draw random numbers from Gaussian distributions that we can use Gibbs sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(I lied to you ; there is an analytic solution for this problem!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BONUS question: look up online what \"conjugate priors\" are appropriate for linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack((xis_true, np.repeat(1, xis_true.size))).T\n",
    "datacov = np.diag(sigma_yis**2.0)\n",
    "A = np.dot(X.T, np.linalg.solve(datacov, X))\n",
    "posterior_mean = np.linalg.solve(A, np.dot(X.T, np.linalg.solve(datacov, yis_noisy[:, None]))).ravel()\n",
    "print('Posterior mean:', posterior_mean)\n",
    "posterior_covariance = np.linalg.inv(np.dot(X.T,  np.linalg.solve(datacov, X)))\n",
    "print('Posterior covariance:', posterior_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important note: you might think sampling is useless here because we know the analytic posterior distribution...\n",
    "    \n",
    "But there is a very wide class of problems and models where you cannot find an analytic solution for the full posterior distribution, but __you can write the conditional posterior distributions on the parameters__! \n",
    "\n",
    "In fact, in hierarchical models, in 99% of cases, we automatically have the conditional distributions, but it is how we construct the model. For this reason, and because Gibbs sampling has an acceptance rate of 1, it is very powerful and popular, and often a default solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EXERCISE: using those results, implement Gibbs sampling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "num_draws = 10000\n",
    "i_draw = 0\n",
    "params_drawn = np.zeros((num_draws, 2))\n",
    "params_drawn[0, :] = posterior_mean + 0.01*np.random.uniform(0, 1, 2)\n",
    "for i_draw in range(1, num_draws):\n",
    "    mu = posterior_mean[0] + posterior_covariance[1, 0] *\\\n",
    "        (params_drawn[i_draw-1, 1] - posterior_mean[1]) / posterior_covariance[1, 1]\n",
    "    cov = posterior_covariance[0, 0] - posterior_covariance[1, 0]**2 / posterior_covariance[1, 1]\n",
    "    params_drawn[i_draw, 0] = mu + np.random.randn() * cov**0.5\n",
    "    \n",
    "    mu = posterior_mean[1] + posterior_covariance[1, 0] *\\\n",
    "        (params_drawn[i_draw, 0] - posterior_mean[0]) / posterior_covariance[0, 0]\n",
    "    cov = posterior_covariance[1, 1] - posterior_covariance[1, 0]**2 / posterior_covariance[0, 0]\n",
    "    params_drawn[i_draw, 1] = mu + np.random.randn() * cov**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].hist2d(sampler.flatchain[:, 0], sampler.flatchain[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[1].hist2d(params_drawn[:, 0], params_drawn[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[0].set_xlabel('slope'); axs[0].set_ylabel('intercept'); axs[1].set_xlabel('slope');\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final words on Gibbs sampling\n",
    "It is great to have an acceptance rate of 1, but we critically need nice conditional distributions, and no nasty degeneracies between the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note to Boris: Go back to slides!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting data with both x and y errors\n",
    "\n",
    "We observe a set of $\\hat{x}_i$ which are noisified versions of the true $x_i$, with Gaussian noise $\\gamma_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sigma_xis = np.repeat(0.1, ndatapoints) * np.random.uniform(0.2, 1.0, ndatapoints)\n",
    "xis_noisy = xis_true + sigma_xis * np.random.randn(xis_true.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.errorbar(xis_noisy, yis_noisy, xerr=sigma_xis, yerr=sigma_yis, fmt='o')\n",
    "plt.xlabel('$x$'); plt.ylabel('$y$'); plt.ylim([y_min, y_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our likelihood is now:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(D| P, M) &= p(\\{ \\hat{y}_i, \\hat{x}_i \\} \\vert \\{\\sigma_i, \\gamma_i, x_i\\}, \\textrm{intercept}, \\textrm{slope}) \\\\\n",
    "&= \\prod_{i=1}^{N} p(\\hat{y}_i \\vert x_i, \\sigma_i, b, m) \\ p(\\hat{x}_i \\vert x_i, \\gamma_i) \\\\\n",
    "& = \\prod_{i=1}^{N} \\mathcal{N}\\left(\\hat{y}_i - m x_i - b; \\sigma^2_i \\right) \\mathcal{N}\\left(\\hat{x}_i - x_i; \\gamma^2_i \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We now have $N$ extra parameters, the $x_i$'s!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The full posterior distribution:\n",
    "\n",
    "$$  p\\bigl( m, s, \\{  x_i \\} \\bigl\\vert  \\{ \\hat{y}_i, \\hat{x}_i, \\sigma_i, \\gamma_i\\} \\bigr) \\ \\propto  \\\n",
    "p\\bigl(\\{ \\hat{y}_i, \\hat{x}_i \\} \\bigl\\vert \\{\\sigma_i, \\gamma_i, x_i\\}, m, s\\bigr) \\ \\ p\\bigl(\\{ x_i \\}, m, s\\bigr) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This is the Curse of Dimensionality v2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One solution : Hamiltonian Monte Carlo\n",
    "\n",
    "Neal's book chapter is a good starting point: https://arxiv.org/abs/1206.1901 \n",
    "\n",
    "Demo: https://chi-feng.github.io/mcmc-demo/app.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Gradients (and hessians) needed! Three strategies:\n",
    "- pen and paper, then home-made implementation\n",
    "- automatic symbolic differentiation\n",
    "- automatic numerical differentition\n",
    "    \n",
    "Always try auto-diff first (e.g., with `autograd`). \n",
    "\n",
    "Large-scale inference (gazilion parameters): try `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analytic marginalization of latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We are only truly interested in the marginalized posterior distribution:\n",
    "\n",
    "$$p\\bigl( m, s \\bigl\\vert  \\{ \\hat{y}_i, \\hat{x}_i, \\sigma_i, \\gamma_i\\} \\bigr) \\ = \\ \\int\\mathrm{d}\\{x_i\\} p\\bigl( m, s, \\{  x_i \\} \\bigl\\vert  \\{ \\hat{y}_i, \\hat{x}_i, \\sigma_i, \\gamma_i\\} \\bigr) \\\\\n",
    " \\propto \\  \\prod_{i=1}^{N} \\int \\mathrm{d}x_i \\mathcal{N}\\left(\\hat{y}_i - m x_i - b; \\sigma^2_i \\right) \\mathcal{N}\\left(\\hat{x}_i - x_i; \\gamma^2_i \\right) \\ \\ p\\bigl(\\{ x_i \\}, m, s\\bigr) \\\\\n",
    " \\propto \\  \\prod_{i=1}^{N} \\mathcal{N}\\left(\\hat{y}_i - m \\hat{x}_i - b; \\sigma^2_i + \\gamma^2_i\\right)  \\ p(s, m) $$\n",
    "\n",
    "with flat uninformative priors on $x_i$'s $p\\bigl(x_i)$.\n",
    "\n",
    "We have eliminated the $x_i$'s!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us do a run with the x's fixed to their noisy values (which is wrong! This is ignoring the x noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nwalkers = 50\n",
    "\n",
    "starting_params = np.random.uniform(0, 1, ndim*nwalkers).reshape((nwalkers, ndim))\n",
    "sampler2 = emcee.EnsembleSampler(nwalkers, ndim, ln_post,\n",
    "                                args=[yis_noisy, xis_noisy, sigma_yis])\n",
    "\n",
    "num_steps = 100\n",
    "pos, prob, state = sampler2.run_mcmc(starting_params, num_steps)\n",
    "num_steps = 1000\n",
    "sampler2.reset()\n",
    "pos, prob, state = sampler2.run_mcmc(pos, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def ln_like(params, observed_yis, observed_xis, yi_uncertainties, xi_uncertainties):\n",
    "    xyi_uncertainties = np.sqrt(xi_uncertainties**2. + yi_uncertainties**2.)\n",
    "    model_yis = model_linear(observed_xis, params[0], params[1])\n",
    "    return np.sum(-0.5 * ((observed_yis - model_yis) / xyi_uncertainties)**2\n",
    "                   - 0.5*np.log(2*np.pi) - np.log(xyi_uncertainties))\n",
    "\n",
    "def ln_prior(params):\n",
    "    if np.any(params < 0) or np.any(params > 1):\n",
    "        return - np.inf\n",
    "    return 0.\n",
    "\n",
    "def ln_post(params, observed_yis, observed_xis, yi_uncertainties, xi_uncertainties):\n",
    "    lnprior_val = ln_prior(params)\n",
    "    if ~np.isfinite(lnprior_val):\n",
    "        return lnprior_val\n",
    "    else:           \n",
    "        lnlike_val = ln_like(params, observed_yis, observed_xis, yi_uncertainties, xi_uncertainties)\n",
    "        return lnprior_val + lnlike_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x0 = np.repeat(0.5, 2)\n",
    "print('Likelihood:', ln_like(x0, yis_noisy, xis_noisy, sigma_yis, sigma_xis))\n",
    "print('Prior:', ln_prior(x0))\n",
    "print('Posterior:', ln_post(x0, yis_noisy, xis_noisy, sigma_yis, sigma_xis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "nwalkers = 50\n",
    "\n",
    "starting_params = np.random.uniform(0, 1, ndim*nwalkers).reshape((nwalkers, ndim))\n",
    "sampler3 = emcee.EnsembleSampler(nwalkers, ndim, ln_post,\n",
    "                                args=[yis_noisy, xis_noisy, sigma_yis, sigma_xis])\n",
    "\n",
    "num_steps = 100\n",
    "pos, prob, state = sampler3.run_mcmc(starting_params, num_steps)\n",
    "num_steps = 1000\n",
    "sampler3.reset()\n",
    "pos, prob, state = sampler3.run_mcmc(pos, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharex=False, sharey=False)\n",
    "axs[0].hist2d(sampler.flatchain[:, 0], sampler.flatchain[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[1].hist2d(sampler2.flatchain[:, 0], sampler2.flatchain[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[2].hist2d(sampler3.flatchain[:, 0], sampler3.flatchain[:, 1], 30, cmap=\"ocean_r\");\n",
    "axs[0].set_title('x true'); \n",
    "axs[1].set_title('x wrongly fixed');  \n",
    "axs[2].set_title('with x errors');\n",
    "axs[0].set_xlabel('slope'); axs[0].set_ylabel('intercept'); \n",
    "axs[1].set_xlabel('slope');\n",
    "axs[2].set_xlabel('slope');\n",
    "for i in range(3):\n",
    "    axs[i].axvline(slope_true)\n",
    "    axs[i].axhline(intercept_true)\n",
    "    axs[i].set_ylim([0, 1])\n",
    "    axs[i].set_xlim([0, 1])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "axs[0].set_xlabel('$x$'); axs[1].set_xlabel('$x$'); axs[0].set_ylabel('$y$');\n",
    "num = 1000\n",
    "y_models = np.zeros((x_grid.size, num))\n",
    "for j, i in enumerate(np.random.choice(np.arange(sampler3.flatchain.shape[0]), num, replace=False)):\n",
    "    y_models[:, j] = model_linear(x_grid, sampler3.flatchain[i, 0], sampler3.flatchain[i, 1])\n",
    "    axs[0].plot(x_grid, y_models[:, j], c='gray', alpha=0.01, zorder=0)\n",
    "axs[1].plot(x_grid, np.mean(y_models, axis=1), c='gray', alpha=1, zorder=0)\n",
    "axs[1].fill_between(x_grid, np.mean(y_models, axis=1)-np.std(y_models, axis=1), \n",
    "            np.mean(y_models, axis=1)+np.std(y_models, axis=1), color='gray', alpha=0.5, zorder=0)\n",
    "axs[0].errorbar(xis_noisy, yis_noisy, xerr=sigma_xis, yerr=sigma_yis, fmt='o', zorder=1)\n",
    "axs[1].errorbar(xis_noisy, yis_noisy, xerr=sigma_xis, yerr=sigma_yis, fmt='o', zorder=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We got around the number of parameters by analytically marginalizing the ones we don't really care about. Sometimes this is not possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extensions\n",
    "\n",
    "- __Automatic differentiation__ with autograd, tensorflow, etc.\n",
    "- __Nested Sampling for nasty distributions and model comparison__. Application: fitting multiple components/lines to a data set.\n",
    "- __Model testing, model comparison__. I have multiple models. Which is the best? Example: fit multiple lines to data.\n",
    "- __Hamiltonian Monte Carlo with quasi-auto-tuning for millions of parameters.__ Application: fitting a line with many latent parameters (x noise, outliers, etc).\n",
    "- __Multi-stage hybrid sampling__: Application: non-linear models with many parameters and complicated gradients. \n",
    "- __Connections to deep machine learning__: Bayesian interpretation of Convolution networks, Adversarial training, deep forward models, etc. TensorFlow.\n",
    "\n",
    "Let me know if you are interested and we will organize a session. A few notebooks and mode advanced examples available on https://ixkael.github.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final thoughts\n",
    "With the right method you can solve problems/models that seem intractable. Don't underestimate yourself! Start small, but be ambitious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
